{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680bc30d",
   "metadata": {},
   "source": [
    "# 🧠 Fine-tuning Llama-3.2-3B-Instruct for Mental Health Support\n",
    "\n",
    "This notebook demonstrates fine-tuning **Llama-3.2-3B-Instruct** using **LoRA adapters** on the **ShivomH/MentalHealth-Support** dataset.\n",
    "\n",
    "## 📋 Prerequisites\n",
    "- GPU: RTX A6000 (48GB) / RTX 6000 Ada (48GB) / RTX 3090 (24GB)\n",
    "- CUDA 11.8+ and PyTorch 2.0+\n",
    "- Hugging Face account for model access\n",
    "- Weights & Biases account for logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fcc9c",
   "metadata": {},
   "source": [
    "## 1. 🔧 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U trl transformers accelerate peft datasets bitsandbytes trl wandb scipy sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e65328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    ")\n",
    "# from trl import SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e56374",
   "metadata": {},
   "source": [
    "## 2. 🔍 GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b02aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Free: {total - allocated:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "# Initial GPU status\n",
    "print_gpu_memory()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47242c07",
   "metadata": {},
   "source": [
    "## 3. ⚙️ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    \n",
    "    # Model & Dataset\n",
    "    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    dataset_name: str = \"enter_dataset_name\"\n",
    "    \n",
    "    # Paths (replace with your chosen directory paths)\n",
    "    output_dir: str = \"directory_name_checkpoints\" #Checkpoints will be stored here.\n",
    "    adapter_dir: str = \"directory_name_LoRA\" #Final LoRA adapter will be stored here.\n",
    "    merged_model_dir: str = \"directory_name_merged\" #Merged model will be stored here.\n",
    "    cache_dir: str = \"./cache\"\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Training Parameters\n",
    "    num_train_epochs: int = 4\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 3e-5\n",
    "    warmup_ratio: float = 0.05\n",
    "    max_seq_length: int = 1024\n",
    "    \n",
    "    # Optimization\n",
    "    optim: str = \"paged_adamw_32bit\"\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    gradient_checkpointing: bool = True\n",
    "    bf16: bool = True\n",
    "    tf32: bool = True\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 50\n",
    "    save_steps: int = 400\n",
    "    eval_steps: int = 200\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    # W&B\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"your_project_name_in_wandb\"\n",
    "    wandb_run_name: str = f\"llama3.2-lora-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub: bool = False\n",
    "    hub_model_id: str = \"your_hub_model_id_to_push_on_huggingface\"\n",
    "    hub_private: bool = True\n",
    "    \n",
    "config = Config()\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5822b9e",
   "metadata": {},
   "source": [
    "## 4. 🔐 Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bfeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face login (required for Llama models)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# You need to accept the Llama license agreement first:\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "\n",
    "# Option 1: Use token from environment\n",
    "os.environ[\"HF_TOKEN\"] = \"your_huggingface_token\"\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# Option 2: Interactive login\n",
    "# login()\n",
    "\n",
    "# W&B login\n",
    "if config.use_wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        name=config.wandb_run_name,\n",
    "        config=config.__dict__\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5c6b0",
   "metadata": {},
   "source": [
    "## 5. 📊 Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset: {config.dataset_name}\")\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nDataset size: {len(dataset)} examples\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(json.dumps(dataset[0], indent=2)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "dataset_split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset):,}\")\n",
    "print(f\"Validation examples: {len(eval_dataset):,}\")\n",
    "\n",
    "# Analyze conversation types\n",
    "def analyze_conversations(dataset):\n",
    "    multi_turn = 0\n",
    "    single_turn = 0\n",
    "    \n",
    "    for example in dataset:\n",
    "        messages = example[\"messages\"]\n",
    "        user_messages = [m for m in messages if m[\"role\"] == \"user\"]\n",
    "        if len(user_messages) > 1:\n",
    "            multi_turn += 1\n",
    "        else:\n",
    "            single_turn += 1\n",
    "    \n",
    "    return multi_turn, single_turn\n",
    "\n",
    "multi, single = analyze_conversations(train_dataset)\n",
    "print(f\"\\nConversation types in training set:\")\n",
    "print(f\"  Multi-turn: {multi:,}\")\n",
    "print(f\"  Single-turn: {single:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18772355",
   "metadata": {},
   "source": [
    "## 6. 🤖 Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer: {config.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    cache_dir=config.cache_dir,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded successfully\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e4587",
   "metadata": {},
   "source": [
    "### **Important Note: The process should be followed in correct sequence, else the model will not be loaded correctly.\n",
    "\n",
    "#### Load the quantized model -> Prepare model for k-bit training -> Gradient Checkpointing -> Apply LoRA to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model: {config.model_name}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=config.cache_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc0bb0",
   "metadata": {},
   "source": [
    "## 7. 🔧 LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=config.target_modules,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params:,} || \"\n",
    "        f\"All params: {all_params:,} || \"\n",
    "        f\"Trainable %: {100 * trainable_params / all_params:.2f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(lora_config)\n",
    "print(\"\\nModel with LoRA:\")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78346a",
   "metadata": {},
   "source": [
    "## 8. 📝 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Apply chat template and tokenize.\n",
    "    Supports multi-turn and single-turn seamlessly.\n",
    "    Returns lists (not tensors) for compatibility with HuggingFace datasets.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        # Ensure it's always a list of dicts\n",
    "        if isinstance(messages, dict):\n",
    "            messages = [messages]\n",
    "\n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize → return lists, not tensors\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=config.max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    # Labels = input_ids\n",
    "    labels = model_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # Mask padding token positions (-100 tells HF Trainer to ignore them in loss)\n",
    "    labels = [\n",
    "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in seq]\n",
    "        for seq in labels\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Test preprocessing on a sample\n",
    "sample = train_dataset.select(range(1))\n",
    "sample_processed = preprocess_function(sample)\n",
    "print(\"Sample preprocessing successful!\")\n",
    "print(f\"Input length: {len(sample_processed['input_ids'][0])}\")\n",
    "print(f\"Attention mask length: {len(sample_processed['attention_mask'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816cf77-5f8e-4c18-a6a0-71c8df163c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_proc = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "eval_dataset_proc = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76228c81",
   "metadata": {},
   "source": [
    "## 9. 🎯 Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d980e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    lr_scheduler_type=config.lr_scheduler_type,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=config.optim,\n",
    "    bf16=config.bf16,\n",
    "    tf32=config.tf32,\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{config.output_dir}/logs\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # W&B\n",
    "    report_to=\"wandb\" if config.use_wandb else \"none\",\n",
    "    run_name=config.wandb_run_name if config.use_wandb else None,\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub=config.push_to_hub,\n",
    "    hub_model_id=config.hub_model_id if config.push_to_hub else None,\n",
    "    hub_private_repo=config.hub_private if config.push_to_hub else None,\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    seed=42,\n",
    "    dataloader_num_workers=4,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "print(f\"\\nEffective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"Total optimization steps: ~{len(train_dataset) // (config.per_device_train_batch_size * config.gradient_accumulation_steps) * config.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21886d4-b6c8-4797-a62b-88e5dc5eb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_proc,\n",
    "    eval_dataset=eval_dataset_proc,\n",
    "    tokenizer=tokenizer,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffa824",
   "metadata": {},
   "source": [
    "## 10. 🚀 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    print(\"\\nSaving final model...\")\n",
    "    trainer.save_model(config.output_dir)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f\"{config.output_dir}/training_metrics.json\", \"w\") as f:\n",
    "        json.dump(trainer.state.log_history, f, indent=2)\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "    trainer.save_model(f\"{config.output_dir}/checkpoint-interrupted\")\n",
    "    \n",
    "finally:\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26094295-7083-4b8e-be1b-3859383d01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RAW EXAMPLE ===\")\n",
    "raw_sample = train_dataset[0]   # first unprocessed example\n",
    "print(json.dumps(raw_sample, indent=2)[:1000])  # pretty print\n",
    "\n",
    "print(\"\\n=== CHAT TEMPLATE EXAMPLE ===\")\n",
    "messages = raw_sample[\"messages\"]\n",
    "templated = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "print(templated)\n",
    "\n",
    "print(\"\\n=== TOKENIZED EXAMPLE ===\")\n",
    "proc_sample = train_dataset_proc[0]  # already preprocessed by map\n",
    "for k, v in proc_sample.items():\n",
    "    print(f\"{k}: {len(v)} -> {v[:20]}\")  # print first 20 tokens\n",
    "\n",
    "print(\"\\n=== DECODED FROM IDS ===\")\n",
    "decoded = tokenizer.decode(proc_sample[\"input_ids\"], skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e7cff",
   "metadata": {},
   "source": [
    "## 11. 📊 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a95679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "import math\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "print(f\"\\nPerplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5f89b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for test prompts...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Test 1:\n",
      "\n",
      "User: I've been feeling really anxious about my upcoming job interview. Any advice?\n",
      "\n",
      "Assistant: I'm sorry to hear that you're feeling anxious about your job interview. It's completely normal to feel that way before a big event like this. Can you tell me a bit more about what specifically is causing your anxiety? Is it the fear of rejection or something else? And what are your goals for this job interview? Understanding your concerns will help me provide you with the best guidance.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Test 2:\n",
      "\n",
      "User: It started about two weeks ago. I keep waking up at 3 AM and can't fall back asleep.\n",
      "\n",
      "Assistant: I'm sorry to hear that you're experiencing disrupted sleep patterns. Have you tried any techniques to help you fall back asleep when you wake up in the middle of the night? Perhaps a warm bath or deep breathing exercises? Have you noticed anything that might be triggering your wakefulness?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_generation(model, tokenizer, prompts, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Evaluate model generation on sample prompts.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for prompt_data in prompts:\n",
    "        messages = prompt_data[\"messages\"]\n",
    "        \n",
    "        # Apply chat template\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=config.max_seq_length\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the assistant's response\n",
    "        response = generated_text.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"input\": messages,\n",
    "            \"response\": response\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    # Single-turn\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a compassionate mental health support assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I've been feeling really anxious about my upcoming job interview. Any advice?\"}\n",
    "        ]\n",
    "    },\n",
    "    # Multi-turn\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a compassionate mental health support assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"I'm having trouble sleeping lately.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"I understand that sleep issues can be really frustrating. Can you tell me more about what's been happening? When did this start, and have you noticed any patterns?\"},\n",
    "            {\"role\": \"user\", \"content\": \"It started about two weeks ago. I keep waking up at 3 AM and can't fall back asleep.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "print(\"Generating responses for test prompts...\\n\")\n",
    "results = evaluate_generation(model, tokenizer, test_prompts)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"\\nUser: {result['input'][-1]['content']}\")\n",
    "    print(f\"\\nAssistant: {result['response']}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaede26f",
   "metadata": {},
   "source": [
    "## 12. 💾 Save LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter only\n",
    "print(f\"Saving LoRA adapter to {config.adapter_dir}\")\n",
    "model.save_pretrained(config.adapter_dir)\n",
    "tokenizer.save_pretrained(config.adapter_dir)\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{config.adapter_dir}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config.__dict__, f, indent=2, default=str)\n",
    "\n",
    "print(\"LoRA adapter saved successfully!\")\n",
    "\n",
    "# Check adapter size\n",
    "import os\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(config.adapter_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.bin') or file.endswith('.safetensors'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "\n",
    "print(f\"\\nAdapter size: {total_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06842795",
   "metadata": {},
   "source": [
    "## 13. 🔀 Merge LoRA with Base Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d137c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to merge LoRA weights with base model for easier deployment\n",
    "merge_model = input(\"Do you want to merge LoRA with base model? (y/n): \").lower() == 'y'\n",
    "\n",
    "if merge_model:\n",
    "    print(\"\\nMerging LoRA adapter with base model...\")\n",
    "    print(\"This will require loading the full model in memory.\")\n",
    "    \n",
    "    # Clear GPU memory first\n",
    "    import gc\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load base model without quantization\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=config.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model_with_lora = PeftModel.from_pretrained(base_model, config.adapter_dir)\n",
    "    \n",
    "    # Merge and unload\n",
    "    merged_model = model_with_lora.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    print(f\"Saving merged model to {config.merged_model_dir}\")\n",
    "    merged_model.save_pretrained(config.merged_model_dir)\n",
    "    tokenizer.save_pretrained(config.merged_model_dir)\n",
    "    \n",
    "    print(\"Merged model saved successfully!\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del base_model, model_with_lora, merged_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b7028",
   "metadata": {},
   "source": [
    "## 14. 🤗 Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to Hugging Face Hub\n",
    "if config.push_to_hub:\n",
    "    print(f\"Pushing model to Hub: {config.hub_model_id}\")\n",
    "    \n",
    "    # Push adapter\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Create model card\n",
    "    model_card = f\"\"\"\n",
    "---\n",
    "library_name: peft\n",
    "base_model: {config.model_name}\n",
    "tags:\n",
    "- mental-health\n",
    "- llama\n",
    "- lora\n",
    "- conversational\n",
    "datasets:\n",
    "- {config.dataset_name}\n",
    "---\n",
    "\n",
    "# Llama-3.2-3B Mental Health Support LoRA\n",
    "\n",
    "This is a LoRA adapter for {config.model_name} fine-tuned on mental health support conversations.\n",
    "\n",
    "## Training Details\n",
    "- **Base Model**: {config.model_name}\n",
    "- **Dataset**: {config.dataset_name}\n",
    "- **LoRA Rank**: {config.lora_r}\n",
    "- **LoRA Alpha**: {config.lora_alpha}\n",
    "- **Learning Rate**: {config.learning_rate}\n",
    "- **Epochs**: {config.num_train_epochs}\n",
    "- **Batch Size**: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{config.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{config.model_name}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, \"{config.hub_model_id}\")\n",
    "```\n",
    "\n",
    "## Disclaimer\n",
    "This model is for research purposes only. It should not replace professional mental health services.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{config.adapter_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    # Push to hub\n",
    "    api.upload_folder(\n",
    "        folder_path=config.adapter_dir,\n",
    "        repo_id=config.hub_model_id,\n",
    "        repo_type=\"model\",\n",
    "        private=config.hub_private\n",
    "    )\n",
    "    \n",
    "    print(f\"Model pushed to: https://huggingface.co/{config.hub_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c7fb9",
   "metadata": {},
   "source": [
    "## 15. 🎮 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac552f87-76c7-4fda-8377-28451fe948b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model for inference\n",
    "print(\"Loading fine-tuned model for inference...\")\n",
    "\n",
    "# Clear memory if needed\n",
    "import gc\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Paths to your saved model/tokenizer\n",
    "model_path = \"merged_model_directory_path\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model (quantization if needed)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.4,\n",
    "    top_p=0.8,\n",
    "    do_sample=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response from the model given messages.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "    \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            top_p=top_p,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant response\n",
    "    response = generated.split(\"assistant\\n\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e65b14-c4e4-498c-b590-704586cc5bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "💬 Mental Health Support Chat\n",
      "Type 'quit' to exit, 'reset' to start a new conversation\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Can you diagnose me with depression? I think I have it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: I'm here to listen and support you. It takes a lot of courage to reach out and acknowledge that you may be experiencing depression. It's important to remember that I'm not a medical professional, so I can't diagnose you. However, I can offer guidance and support as you explore your feelings and thoughts.\n",
      "\n",
      "When you say you think you have depression, what are some of the symptoms you've been experiencing? Can you tell me more about what you've been going through? This will help us understand your situation better. Is there anything specific that has triggered these feelings? And how long have you been feeling this way? Understanding the context can be helpful in finding ways to cope. Let's take it at your pace and work together to find some solutions. What do you think would be helpful for you right now?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye! Take care.\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat interface\n",
    "print(\"=\"*60)\n",
    "print(\"💬 Mental Health Support Chat\")\n",
    "print(\"Type 'quit' to exit, 'reset' to start a new conversation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System prompt\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are a compassionate and supportive mental health assistant. \n",
    "    Provide helpful, empathetic responses while maintaining appropriate boundaries. \n",
    "    Never reveal personal info like names, phone numbers, or addresses.\n",
    "    Remember: You are not a replacement for professional mental health services.\"\"\"\n",
    "}\n",
    "\n",
    "# Conversation history\n",
    "conversation = [system_message]\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYou: \").strip()\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Goodbye! Take care.\")\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'reset':\n",
    "        conversation = [system_message]\n",
    "        print(\"Conversation reset.\")\n",
    "        continue\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    # Add user message\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # Generate assistant reply\n",
    "    reply = generate_response(model, tokenizer, conversation)\n",
    "    \n",
    "    # Add assistant message to history\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    # Print assistant response\n",
    "    print(f\"\\nAssistant: {reply}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
